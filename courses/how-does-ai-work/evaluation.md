# Episode Evaluation for "How Does AI Work?"

## Evaluation Methodology

For each episode, I'm looking for:
- ✅ **Clear expert credentials** (researcher, professor, company founder/employee at major AI lab)
- ✅ **Interview format** (not solo host monologue)
- ✅ **Recency** (2024-2025 preferred, or timeless foundational content)
- ✅ **Technical depth** (discusses architectures, not just applications)
- ✅ **Expertise signals** in description

Rating system:
- ⭐⭐⭐ EXCELLENT - Use these first
- ⭐⭐ GOOD - Solid backup
- ⭐ ACCEPTABLE - Use only if needed
- ❌ REJECT - Do not use

---

## ⭐⭐⭐ EXCELLENT EPISODES

### BEGINNER LEVEL (Foundational Concepts)

#### 1. Demis Hassabis: DeepMind - Lex Fridman Podcast
- **Episode ID**: 21808243
- **Podcast ID**: 22625
- **Podcast**: Lex Fridman Podcast
- **Title**: "#299 – Demis Hassabis: DeepMind"
- **Guest**: Demis Hassabis, CEO and co-founder of DeepMind
- **Expertise Signals**:
  - CEO and co-founder of DeepMind (acquired by Google)
  - Leading AI research organization
  - Behind AlphaGo, AlphaFold breakthroughs
- **Topics**: Turing Test, video games, AlphaFold, solving intelligence, consciousness, origin of life
- **Why Beginner**: Accessible conversation about AI fundamentals from one of the field's leading figures
- **Recency**: 2022-07-01
- **Rating**: ⭐⭐⭐ EXCELLENT

#### 2. Yann LeCun - Episode 17
- **Episode ID**: 2954122
- **Podcast ID**: 57281
- **Podcast**: (Appears to be AI-focused podcast)
- **Title**: "Episode 17 - Yann Lecun"
- **Guest**: Yann LeCun
- **Expertise Signals**:
  - Turing Award winner (highest prize in computer science)
  - One of the brightest minds in machine learning
  - Work behind computer vision systems (face recognition, self-driving cars)
  - Recently won Turing Award
- **Topics**: First computer, how music led him into CS, self-supervised learning, path to human-level intelligence
- **Why Beginner**: Covers foundational journey and accessible concepts about AI development
- **Rating**: ⭐⭐⭐ EXCELLENT

#### 3. Dario Amodei CEO of Anthropic - In Good Company
- **Episode ID**: 57172723
- **Podcast ID**: 201406
- **Podcast**: In Good Company (Norges Bank Investment Management)
- **Title**: "Dario Amodei CEO of Anthropic: Claude, New models, AI safety and Economic impact"
- **Guest**: Dario Amodei, CEO of Anthropic
- **Expertise Signals**:
  - CEO of Anthropic (company behind Claude)
  - Discusses latest advancements in AI
  - Focus on responsible scaling
  - Upcoming models and economic impact
- **Topics**: AI models scale, Claude, AI safety, economic impact, responsible scaling
- **Why Beginner**: Covers current state of AI from industry leader perspective
- **Recency**: Recent (likely 2024)
- **Rating**: ⭐⭐⭐ EXCELLENT

---

### INTERMEDIATE LEVEL (Core Mechanics/Systems)

#### 4. Neel Nanda - Mechanistic Interpretability - Machine Learning Street Talk
- **Episode ID**: 30854956
- **Podcast ID**: 213859
- **Podcast**: Machine Learning Street Talk
- **Title**: "Neel Nanda - Mechanistic Interpretability"
- **Guest**: Neel Nanda, researcher at DeepMind
- **Expertise Signals**:
  - Researcher at DeepMind working on mechanistic interpretability
  - PhD-level work on understanding ML models
  - Published research on transformer circuits, induction heads, grokking
  - Technical expertise in reverse engineering neural networks
- **Topics**: Mechanistic interpretability, reverse engineering neural networks, superposition, induction heads, transformer circuits
- **Why Intermediate**: Deep technical dive into how models actually work internally
- **Rating**: ⭐⭐⭐ EXCELLENT

#### 5. Dario Amodei, Amanda Askell, Chris Olah - Lex Fridman #452
- **Episode ID**: 78099403
- **Podcast ID**: 22625
- **Podcast**: Lex Fridman Podcast
- **Title**: "#452 – Dario Amodei: Anthropic CEO on Claude, AGI & the Future of AI & Humanity"
- **Guests**:
  - Dario Amodei (CEO of Anthropic)
  - Amanda Askell (AI researcher, Claude's character)
  - Chris Olah (AI researcher, mechanistic interpretability)
- **Expertise Signals**:
  - Dario: CEO of Anthropic, created Claude
  - Amanda: AI researcher working on Claude's personality
  - Chris Olah: Legendary interpretability researcher
- **Topics**: Scaling laws, Claude 3.5 Sonnet, Claude 4.0, RLHF, Constitutional AI, mechanistic interpretability, features/circuits/universality, superposition
- **Why Intermediate**: Technical discussion of training methods and model architecture from builders
- **Recency**: Recent (episode #452 suggests 2024)
- **Rating**: ⭐⭐⭐ EXCELLENT

#### 6. Dario Amodei (Anthropic CEO) - Dwarkesh Podcast
- **Episode ID**: 32253826
- **Podcast ID**: 202059
- **Podcast**: Dwarkesh Podcast
- **Title**: "Dario Amodei (Anthropic CEO) — The hidden pattern behind every AI breakthrough"
- **Guest**: Dario Amodei, CEO of Anthropic
- **Expertise Signals**:
  - CEO of Anthropic
  - Deep technical knowledge of scaling
  - Built major AI systems
- **Topics**: Scaling, language, economic usefulness, alignment & mechanistic interpretability, training inefficiencies
- **Why Intermediate**: Technical discussion of what makes AI work and scale
- **Recency**: 2023-08-08
- **Rating**: ⭐⭐⭐ EXCELLENT

#### 7. Dwarkesh Patel: The Scaling Era of AI is Here - Limitless/Bankless
- **Episode ID**: 95698105
- **Podcast ID**: 690983
- **Podcast**: Limitless (Bankless)
- **Title**: "Dwarkesh Patel: The Scaling Era of AI is Here"
- **Guest**: Dwarkesh Patel
- **Expertise Signals**:
  - Renowned podcaster who has interviewed Mark Zuckerberg, Ilya Sutskever, Dario Amodei
  - Author of "The Scaling Era" book
  - Deep understanding of AI development and scaling laws
  - TIME's 100 most influential people in AI (2024)
- **Topics**: Scaling era, neural networks, transformers, scaling laws, compute, AGI timelines
- **Why Intermediate**: Explains the scaling paradigm that drives modern AI
- **Recency**: 2024/2025 (recent)
- **Rating**: ⭐⭐⭐ EXCELLENT

---

### ADVANCED LEVEL (Research Frontiers/Debates)

#### 8. Neel Nanda - Mechanistic Interpretability (Sparse Autoencoders) - MLST
- **Episode ID**: 79647601
- **Podcast ID**: 213859
- **Podcast**: Machine Learning Street Talk
- **Title**: "Neel Nanda - Mechanistic Interpretability (Sparse Autoencoders)"
- **Guest**: Neel Nanda, senior research scientist at Google DeepMind
- **Expertise Signals**:
  - Senior research scientist at Google DeepMind
  - Leads mechanistic interpretability team
  - Age 25, already prominent voice in AI research
  - Pure mathematics degree from Cambridge (2020)
- **Topics**: Sparse autoencoders, chain-of-thought reasoning, neural network internal representations, feature learning, scaling laws
- **Why Advanced**: Cutting-edge research on understanding model internals
- **Recency**: Recent (sparse autoencoders are cutting-edge)
- **Rating**: ⭐⭐⭐ EXCELLENT

#### 9. Paul Christiano - How to Solve AI Alignment - Bankless
- **Episode ID**: 29552823
- **Podcast ID**: 184019
- **Podcast**: Bankless
- **Title**: "168 - How to Solve AI Alignment with Paul Christiano"
- **Guest**: Paul Christiano
- **Expertise Signals**:
  - Runs the Alignment Research Center (non-profit)
  - Previously ran language model alignment team at OpenAI
  - Invented RLHF (Reinforcement Learning from Human Feedback)
  - Mission: align future ML systems with human interests
- **Topics**: AI alignment problem, scalable oversight, training AIs to be honest, technical solutions, RLHF
- **Why Advanced**: Deep dive into alignment research and unsolved problems
- **Recency**: 2023-04-24
- **Rating**: ⭐⭐⭐ EXCELLENT

#### 10. Paul Christiano — Preventing an AI takeover - Dwarkesh
- **Episode ID**: 35826766
- **Podcast ID**: 202059
- **Podcast**: Dwarkesh Podcast
- **Title**: "Paul Christiano — Preventing an AI takeover"
- **Guest**: Paul Christiano
- **Expertise Signals**:
  - World's leading AI safety researcher
  - Invented RLHF
  - Runs Alignment Research Center
- **Topics**: AGI timelines, misalignment and takeover, responsible scaling policies, alignment research, dual-use concerns
- **Why Advanced**: Discusses frontier AI safety challenges
- **Recency**: 2023-10-31
- **Rating**: ⭐⭐⭐ EXCELLENT

#### 11. Eliezer Yudkowsky vs George Hotz AI Safety Debate - Dwarkesh
- **Episode ID**: 32483383
- **Podcast ID**: 202059
- **Podcast**: Dwarkesh Podcast
- **Title**: "George Hotz vs Eliezer Yudkowsky AI Safety Debate"
- **Guests**: Eliezer Yudkowsky and George Hotz
- **Expertise Signals**:
  - Eliezer Yudkowsky: AI safety researcher, co-founder of Machine Intelligence Research Institute (MIRI), legendary thinker in AI safety
  - George Hotz: Founded comma.ai (self-driving), hacker, engineer
- **Topics**: AI safety positions, existential risk, alignment debates
- **Why Advanced**: Explores deep disagreements about AI risk and safety
- **Rating**: ⭐⭐⭐ EXCELLENT

#### 12. Eliezer Yudkowsky and Stephen Wolfram on AI X-risk - MLST
- **Episode ID**: 78098938
- **Podcast ID**: 213859
- **Podcast**: Machine Learning Street Talk
- **Title**: "Eliezer Yudkowsky and Stephen Wolfram on AI X-risk"
- **Guests**: Eliezer Yudkowsky and Stephen Wolfram
- **Expertise Signals**:
  - Eliezer: AI safety expert, MIRI co-founder
  - Stephen Wolfram: Computational scientist, creator of Mathematica and Wolfram Alpha, studied computational irreducibility
- **Topics**: AI safety, consciousness, computational irreducibility, intelligence, existential risks, goal-directed behavior
- **Why Advanced**: Philosophical and technical debate about AI risks
- **Rating**: ⭐⭐⭐ EXCELLENT

---

## ⭐⭐ GOOD EPISODES (Solid Backups)

### Additional Strong Candidates

#### 13. Daniela and Dario Amodei on Anthropic - Future of Life Institute
- **Episode ID**: 41495882
- **Podcast ID**: 300984
- **Podcast**: Future of Life Institute Podcast
- **Title**: "Daniela and Dario Amodei on Anthropic"
- **Guests**: Daniela and Dario Amodei (Anthropic founders)
- **Expertise Signals**:
  - Founders of Anthropic
  - Building reliable, interpretable, steerable AI
  - Public benefit corporation structure
- **Topics**: Anthropic's mission, research strategy, transformer circuits, AI safety, public benefit structure
- **Why Good**: Foundational discussion of Anthropic's approach
- **Rating**: ⭐⭐ GOOD

#### 14. RLHF (Reinforcement Learning from Human Feedback) - Educational Episode
- **Episode ID**: 87344575
- **Podcast ID**: 662804
- **Podcast**: (AI education podcast)
- **Title**: "RLHF (Reinforcement Learning from Human Feedback)"
- **Expertise Signals**: Educational content about RLHF
- **Topics**: RLHF pipeline, reward model, KL divergence, preference fine-tuning
- **Why Good**: Focused technical explanation of key training technique
- **Rating**: ⭐⭐ GOOD

#### 15. Stable Diffusion Deep Dive - Practical AI
- **Episode ID**: 23600987
- **Podcast ID**: 57254
- **Podcast**: Practical AI (Changelog)
- **Title**: "Stable Diffusion"
- **Guests**: Chris Benson and Daniel Whitenack
- **Expertise Signals**: Technical podcast hosts with ML expertise
- **Topics**: Diffusion models, stable diffusion architecture, image generation, U-Net
- **Why Good**: Technical explanation of diffusion models
- **Rating**: ⭐⭐ GOOD

---

## ADDITIONAL SEARCH RESULTS TO CONSIDER

### Transformer Architecture Episodes
- Several episodes found on transformer architecture, but many lack clear expert credentials or are too basic/theoretical

### AGI Timeline Episodes
- Multiple episodes on AGI timelines found, but need more specific expert interviews

### Neural Network Training Episodes
- Found gradient descent and backpropagation episodes, but most are educational explainers rather than expert interviews

---

## COURSE STRUCTURE RECOMMENDATIONS

### BEGINNER (6 episodes)
Focus: What is AI, how neural networks work, what transformers are, how models learn

Recommended:
1. **Demis Hassabis: DeepMind** (Lex Fridman) - Big picture from AI pioneer
2. **Yann LeCun** - Turing Award winner on AI foundations
3. **Dario Amodei: Anthropic CEO** (In Good Company) - Current state of AI 2024
4. Need 3 more beginner-accessible expert interviews

### INTERMEDIATE (6 episodes)
Focus: How training works, scaling laws, RLHF, model architectures, diffusion models

Recommended:
1. **Dario Amodei: Hidden Pattern Behind AI** (Dwarkesh) - Scaling and training
2. **Dario Amodei, Chris Olah, Amanda Askell** (Lex Fridman #452) - Claude training, Constitutional AI, interpretability
3. **Dwarkesh Patel: Scaling Era** - Scaling laws and emergence
4. **Neel Nanda: Mechanistic Interpretability** (MLST) - How models work internally
5. Need 2 more intermediate episodes

### ADVANCED (6 episodes)
Focus: AI safety, alignment, interpretability research, existential risk, future of AI

Recommended:
1. **Neel Nanda: Sparse Autoencoders** (MLST) - Cutting-edge interpretability
2. **Paul Christiano: AI Alignment** (Bankless) - Alignment solutions
3. **Paul Christiano: Preventing AI Takeover** (Dwarkesh) - AI safety research
4. **Eliezer Yudkowsky vs George Hotz** (Dwarkesh) - Safety debate
5. **Eliezer Yudkowsky & Stephen Wolfram** (MLST) - X-risk philosophical debate
6. Need 1 more advanced episode

---

## GAPS TO FILL

Need to find:
1. **Beginner**: 3 more episodes
   - Good accessible episode on what neural networks are
   - Episode on how transformers revolutionized AI (attention mechanism)
   - Episode on GPT evolution or LLMs basics

2. **Intermediate**: 2 more episodes
   - Technical episode on diffusion models (image generation)
   - Episode on multimodal AI or specific architecture innovations

3. **Advanced**: 1 more episode
   - Could use another on AGI timelines or capabilities research

---

## QUALITY ASSURANCE NOTES

✅ **Strengths so far**:
- Excellent expert credentials across the board
- Mix of industry (Anthropic, DeepMind, OpenAI) and research (MIRI, ARC)
- Recent episodes (2022-2024)
- Technical depth without being inaccessible
- Diversity of podcasts

⚠️ **Challenges**:
- Need more beginner-level content that's still expert-led
- Most excellent episodes are intermediate-advanced
- Need to ensure 18 different podcasts (currently using some podcasts multiple times)

---

## NEXT STEPS

1. Search for more beginner-friendly expert episodes
2. Ensure podcast diversity (18 different podcasts)
3. Verify episode IDs and podcast IDs are correct
4. Create final `course.txt` with justifications
5. Build JSON file
